# based on https://github.com/Project-MONAI/GenerativeModels/blob/main/tutorials/generative/2d_diffusion_autoencoder/2d_diffusion_autoencoder_tutorial.ipynb

from typing import Optional, Sequence

import numpy as np
import torch
import torch.nn as nn
import tqdm
from bioio.writers import OmeTiffWriter
from generative.inferers import DiffusionInferer
from generative.networks.schedulers.ddim import DDIMScheduler
from monai.inferers import Inferer
from torchmetrics import MeanMetric

from cyto_dl.models.base_model import BaseModel
from cyto_dl.models.im2im.utils.postprocessing import detach, metatensor_batch_to_tensor


class DiffusionAutoEncoder(BaseModel):
    """
    [DiffusionAutoencoder](https://arxiv.org/abs/2111.15640) for representation learning
    """

    def __init__(
        self,
        *,
        semantic_encoder: nn.Module,
        autoencoder: nn.Module,
        spatial_inferer: Inferer,
        image_shape: Sequence[int],
        condition_key: str,
        diffusion_key: Optional[str] = None,
        n_train_steps: int = 1000,
        save_dir="./",
        save_images_every_n_epochs: int = 1,
        batch_is_cond: bool = False,
        n_noise_samples: Optional[int] = 1,
        train_encoder: bool = True,
        **base_kwargs,
    ):
        """
        Parameters
        ----------
        semantic_encoder: nn.Module
            model network to encode the condition image
        autoencoder: nn.Module
            model network to denoise the diffusion image (conditioned on the latent generated by the semantic encoder)
        spatial_inferer: Inferer
            Inferer to use for splitting large images into patches during inference
        image_shape: Sequence[int]
            shape of the input images
        condition_key: str
            key to access condition images in batch
        diffusion_key: Optional[str]
            key to access diffusion images in batch. If None, defaults to condition_key
        n_train_steps: int
            number of noise steps used during diffusion model training
        save_dir="./"
            directory to save images during training and validation
        save_images_every_n_epochs: int
            Image saving frequency
        batch_is_cond: bool
            Whether the batch is a tensor of latent features for conditioning the diffusion model. Only used during inference. If True, a latent walk is performed, otherwise latent features are extracted from the condition image using the semantic encoder.
        n_noise_samples: Optional[int]
            Number of noise samples to average for latent walk
        train_encoder: bool
            Whether to train the semantic encoder
        **base_kwargs:
            Additional arguments passed to BaseModel
        """

        _DEFAULT_METRICS = {
            "train/loss": MeanMetric(),
            "val/loss": MeanMetric(),
            "test/loss": MeanMetric(),
        }

        metrics = base_kwargs.pop("metrics", _DEFAULT_METRICS)
        super().__init__(metrics=metrics, **base_kwargs)
        self.diffusion_key = diffusion_key or condition_key
        self.autoencoder = autoencoder
        if not train_encoder:
            for param in semantic_encoder.parameters():
                param.requires_grad = False
        self.semantic_encoder = semantic_encoder

        self.scheduler = DDIMScheduler(n_train_steps)
        self.inferer = DiffusionInferer(self.scheduler)
        self.spatial_inferer = spatial_inferer

        self.loss = torch.nn.MSELoss()

    def configure_optimizers(self):
        params = list(self.autoencoder.parameters())
        if self.hparams.train_encoder:
            params += list(self.semantic_encoder.parameters())

        opt = self.optimizer["generator"](params)
        sched = self.lr_scheduler["generator"](optimizer=opt)
        return [opt], [sched]

    def forward(self, x_cond, x_diff):
        noise = torch.randn_like(x_diff, device=x_diff.device)
        timesteps = torch.randint(
            0,
            self.inferer.scheduler.num_train_timesteps,
            (x_diff.shape[0],),
            device=x_diff.device,
            dtype=torch.long,
        )
        # latent is B x C x 1
        latent = self.semantic_encoder(x_cond).unsqueeze(2)
        noise_pred = self.inferer(
            inputs=x_diff,
            diffusion_model=self.autoencoder,
            noise=noise,
            timesteps=timesteps,
            condition=latent,
        )
        return noise, noise_pred, latent

    def _generate_image(self, noise, cond):
        with torch.no_grad():
            sample, intermediates = self.inferer.sample(
                input_noise=noise,
                diffusion_model=self.autoencoder,
                scheduler=self.scheduler,
                save_intermediates=True,
                conditioning=cond,
                verbose=False,
                intermediate_steps=1,
            )
        # during training, final image is all-nan, try to return last non-nan image
        if torch.any(torch.isnan(sample)):
            while torch.any(torch.isnan(sample)) and len(intermediates) > 0:
                sample = intermediates.pop(-1)
        return sample

    def generate_image(self, stage, cond_img, diff_img):
        """Save the sequence of denoising steps."""
        self.scheduler.set_timesteps(num_inference_steps=100)
        with torch.no_grad():
            cond = self.semantic_encoder(cond_img).unsqueeze(2)
        noise = torch.randn_like(cond_img, device=self.device)
        sample = self._generate_image(noise, cond)

        for img, name in zip([cond_img, diff_img, sample], ["cond", "diff", "recon"]):
            OmeTiffWriter.save(
                uri=f"{self.hparams.save_dir}/{self.trainer.current_epoch}_{stage}_{name}.tiff",
                data=detach(img).astype(float),
            )

    def model_step(self, stage, batch, batch_idx):
        batch = metatensor_batch_to_tensor(batch)
        cond_img = batch[self.hparams.condition_key]
        diff_img = batch[self.diffusion_key]
        noise, noise_pred, latent = self.forward(cond_img, diff_img)
        if (
            (self.trainer.current_epoch % self.hparams.save_images_every_n_epochs)
            == batch_idx
            == 0
        ):
            self.generate_image(stage, cond_img[:1], diff_img[:1])
        diffusion_loss = self.loss(noise, noise_pred)

        return {"loss": diffusion_loss}, latent, None

    def validation_step(self, batch, batch_idx):
        loss, preds, targets = self.model_step("val", batch, batch_idx)
        self.compute_metrics(loss, preds, targets, "val")
        return loss, preds

    def generate_from_latent(self, cond, save_name):
        self.scheduler.set_timesteps(num_inference_steps=100)
        with torch.no_grad():
            recon = None
            for _ in tqdm.tqdm(range(int(self.hparams.n_noise_samples)), desc="Sampling"):
                # keep noise constant across walk for consistency
                noise = torch.stack(
                    [torch.randn([1] + self.hparams.image_shape, device=self.device)]
                    * cond.shape[0]
                )
                sample = self._generate_image(noise, cond.unsqueeze(2))
                recon = sample if recon is None else recon + sample
            # average reconstruction across samples
            recon /= self.hparams.n_noise_samples
        OmeTiffWriter.save(
            uri=f"{self.hparams.save_dir}/{save_name}.tiff", data=detach(recon).astype(float)
        )

    def _generate_and_re_embed(self, z, batch_idx):
        """Check cycle consistency between latent features extracted from a real image and the
        latent features extracted from an image generated by those latent features."""

        self.scheduler.set_timesteps(num_inference_steps=100)
        with torch.no_grad():
            cond = self.semantic_encoder(z).unsqueeze(2)
            noise = torch.randn_like(z)
            gen = self.inferer.sample(
                input_noise=noise,
                diffusion_model=self.autoencoder,
                scheduler=self.scheduler,
                save_intermediates=False,
                conditioning=cond,
                verbose=False,
            )
            re_embed = self.semantic_encoder(gen)
        np.save(f"{self.hparams.save_dir}/{batch_idx}_re_embed.tif", detach(re_embed))
        np.save(f"{self.hparams.save_dir}/{batch_idx}_original.tif", detach(cond).squeeze(2))

    def encode_image(self, x):
        with torch.no_grad():
            z, loc = self.spatial_inferer(x, self.semantic_encoder)
        return z, loc

    def predict_step(self, batch, batch_idx):
        if self.hparams.batch_is_cond:
            self.latent_walk(batch[self.hparams.condition_key].squeeze(0), "latent_walk")
            return None, None
        meta = batch[self.hparams.condition_key].meta
        batch = metatensor_batch_to_tensor(batch)
        with torch.no_grad():
            z, loc = self.encode_image(batch[self.hparams.condition_key])
        meta.update(loc)
        return detach(z), meta
